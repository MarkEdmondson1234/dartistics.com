---
title: "Adobe Analytics - R Connector"
author: "Mark Edmondson"
date: "26 September 2016"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Initial Setup

First download and install [R](https://cran.r-project.org/mirrors.html) from CRAN and [RStudio](https://www.rstudio.com/)

* CRAN is the organisation that maintains and distributes R
* R is the programming language itself
* RStudio is a program that makes using R a lot easier

# Installation

```r
## This installs a package for downloading Adobe Analytics data
install.packages("RSiteCatalyst")
```

Check out the resources made by its creator, [Randy Zwitch](http://randyzwitch.com/rsitecatalyst/)

## Recording secret keys

I save these values to a `.Renviron` file in my home directory to keep them safe and call them via the the R command `Sys.envir`.

This keeps them out of your working folder that may be shared and compromise your security.

### Example .Renviron file

This is in my root directory: `~/.Renviron`

```
ADOBE_KEY="a7xxxxx639-iih-nordic-r"
ADOBE_SECRET="2eadxxxxx1495ea49"
ADOBE_KEY_OLD="mark:XXXX"
ADOBE_SECRET_OLD="74b46625xxxxx7e89dbe6e0"
```

# Authentication (New)

This is the new OAuth2 method of authentication.  

Use this one if you can, but at time of writing MobileLife does not have this activated for its account. 

Link your Adobe Analytics login to the Adobe ID account to use this method of signin. 

## Create Application

If you have no application for your Adobe Analytics account, here is how to make one:

1. Create an app by visiting developer console and navigating to [Developer > Applications](https://marketing.adobe.com/developer/applications)
2. Select "Web Server (Authorization Code)"
3. Name the application
4. Leave the Redirect Uri field blank.

![setting-up-application](setting-up-adobe-application.png)

You should then see this screen to grant permission after running `SCAuth`

![adobe permission](adobe-app-oauth-permission.png)

Authentication is done via the below.

The `SCAuth()` function will open your browser or read the existing `auth-adobe` file.

```r
library(RSiteCatalyst)

## New OAuth method
key    <- Sys.getenv("ADOBE_KEY")
secret <- Sys.getenv("ADOBE_SECRET")

SCAuth(key, secret, 
       company = "XXXXX", 
       token.file = "auth-adobe", 
       auth.method = "OAUTH2")
```

# Authentication (Old)

This is the deprecated method and will dissappear 'soon'.

Get the Web Service key under the Adobe Analytics Account Information page.

* User Name is used as the key
* Shared Secret is used as the secret

![legacy webservice](legacy-authentication-adobe.png)

If you use this method, then you authenticate via:

```{r}
library(RSiteCatalyst)

key <- Sys.getenv("ADOBE_KEY_OLD")
secret <- Sys.getenv("ADOBE_SECRET_OLD")

SCAuth(key, secret)
```

The `SCAuth()` function will return `[1] "Credentials Saved in RSiteCatalyst Namespace."`

# Fetching Report Suites

Once you have setup your authentication method, call the SCAuth() function at the start of every new R session. 

You can test the connection by calling the account report suites via:

```{r}
suites <- GetReportSuites()
head(suites)
```

# Various other meta data

These are useful to keep for calling data reports.

Descriptions of various elements, metrics etc. for the [1.4 version of the API is found here.](https://marketing.adobe.com/developer/documentation/analytics-reporting-1-4/elements)

```{R metadata, message=FALSE, warning=FALSE}
segments <- GetSegments("grecodemystified")
segments

calc_metrics <- GetCalculatedMetrics("grecodemystified")
calc_metrics

classifications <- GetClassifications("grecodemystified")
classifications

elements <- GetElements("grecodemystified")
head(elements)

metrics <- GetMetrics("grecodemystified")
head(metrics)

props <- GetProps("grecodemystified")
head(props)

evars <- GetEvars("grecodemystified")
head(evars)
```

Read the RSiteCatalyst help files for a full function list.

# Fetching Data Reports

There are 5 different types of Adobe Analytics reports that can be pulled out via the API:

* Overtime - `QueueOvertime()`
* Ranked - `QueueRanked()`
* Trended - `QueueTrended()`
* Pathing - `QueuePathing()`
* Fallout - `QueueFallout()`

See the characteristics of the [reports here](https://marketing.adobe.com/developer/documentation/sitecatalyst-reporting/r-queueovertime)

## Advanced API fetching

If you are not covered by the existing template calls, other API calls can be made via `SubmitJsonQueueReport` if you build the JSON yourself.  It is suggested to use R lists and the library `jsonlite` to create these.

## API response time

* Response times for the API is variable
* It can take anywhere between 10 seconds and 5 minutes or even timeout.
* Size of the requested report doesn't make much impact (within reason)
* Recommend a function to wrap around API calls to retry if an error detected if used in automatic scripts.
* The first call is most expensive - subsequent calls are cached.

# Demo API calls

We set some common parameters here:

```{r}
date.from <- "2016-03-01"
date.to <- "2016-03-8"
reportsuite.id <- "grecodemystified"
```

## Overtime

Returns an overtime report.  This is similar to the key metrics report in that the only granularity allowed is time.

QueueOvertime requires a start and end date, a reportsuite ID, and a character vector of metrics.

```{r, warning=FALSE}
metrics <- c("visits","uniquevisitors","pageviews")

## wrap in system.time to report how long it takes
overtime.data <- QueueOvertime(reportsuite.id, date.from, date.to, metrics)
head(overtime.data)
```

You may also wish to set any of the 5 optional named parameters.

```{r, warning=FALSE}
metrics <- c("visits","uniquevisitors","pageviews")
date.granularity <- "hour"
segment.id <- "Visit_Natural_Search"
anomaly.detection <- FALSE
data.current <- TRUE
expedite <- FALSE ## only can set to TRUE if you have permission

overtime.data <- QueueOvertime(reportsuite.id, 
                                           date.from, date.to, 
                                           metrics,
                                           date.granularity = date.granularity,
                                           segment.id = segment.id,
                                           anomaly.detection = anomaly.detection,
                                           data.current = data.current,
                                           expedite = expedite)

```


### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(overtime.data))
```

```{r}
str(overtime.data)
```

## Ranked

Returns a ranked report. This is an ordered list of elements and associated metrics with no time granularity.

QueueRanked requires a start and end date, a reportsuite ID, a character vector of elements and a character vector of metrics.

```{r}
metrics <- c("visits","uniquevisitors","pageviews","event1")
elements <- c("page","geoCountry","geoCity")

ranked.data <- QueueRanked(reportsuite.id, date.from, date.to, metrics, elements)
```

You may also wish to set any of the 6 optional named parameters. The 1.4 API only supports this for the first element specified. In the example below, selected applies to the first element, page.

```{r, warning=FALSE}
metrics <- c("visits","uniquevisitors","pageviews","event1","event2")
elements <- c("page","geoCountry","geoCity")
top <- 10
start <- 10
selected <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
              "excel-tips/excel-dynamic-named-ranges-never-manually-updating-your-charts-2/")
segment.id <- ""
data.current <- TRUE
expedite <- FALSE ## only can set to TRUE if you have permission

ranked.data <- QueueRanked(reportsuite.id, 
                           date.from, date.to,
                           metrics,
                           elements,
                           top = top,
                           start = start,
                           selected = selected,
                           segment.id = segment.id,
                           data.current = data.current,
                           expedite = expedite)

unique(ranked.data$page)

```

### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(ranked.data))
```

```{r}
str(ranked.data)
```

## Trended

Returns a trended report. This is an ordered list of elements and associated metrics with time granularity.

QueueTrended requires a start and end date, a reportsuite ID, a character vector of elements and a character vector of metrics.

```{r}
metrics <- c("visits","uniquevisitors","pageviews","event1")
elements <- c("page","geoCountry","geoCity")

trended.data <- QueueTrended(reportsuite.id, date.from, date.to, metrics, elements)

```

You may also wish to set any of the 7 optional named parameters. As with `QueueRanked()` the 1.4 API only supports selected for the first element specified.

```{r}
metrics <- c("visits","uniquevisitors","pageviews","event1")
elements <- c("page","geoCountry","geoCity")
top <- 100
start <- 100
selected <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
              "excel-tips/excel-dynamic-named-ranges-never-manually-updating-your-charts-2/")
date.granularity <- "hour"
segment.id <- ""
data.current <- TRUE
expedite <- FALSE

trended.data <- QueueTrended(reportsuite.id, 
                            date.from, date.to,
                            metrics,
                            elements,
                            top = top,
                            start = start,
                            selected = selected,
                            segment.id = segment.id,
                            data.current = data.current,
                            expedite = expedite)
```

### Data format

Report data is returned with the following R classes:

```{r}
## to print in table convert POSIXlt to Date 
trended.data$datetime <- as.Date(trended.data$datetime)
knitr::kable(head(trended.data))
```

```{r}
str(trended.data)
```

## Pathing

Returns a pathing report. This is an ordered list of paths matching the specified pattern.

QueuePathing requires a start and end date, a reportsuite ID, a single element, a single metric and a pattern of element values that defined the path.

```{r}
metric <- "pageviews"
element <- "page"
pattern <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
             "::anything::",
             "::anything::")

pathing.data <- QueuePathing(reportsuite.id, 
                             date.from, date.to, 
                             metric, 
                             element, 
                             pattern)
```

### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(pathing.data))
```

```{r}
str(pathing.data)
```

## Fallout

Returns a fallout report. This is a pathed list of elements, with fallout values for each.

QueuePathing requires a start and end date, a reportsuite ID, a single element, a character vector of metrics and a character vector of element values that defined the checkpoints.

```{r}
metrics <- "pageviews"
element <- "page"
checkpoints <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
                 "excel-tips/excel-dropdowns-done-right/",
                 "excel-tips/excel-dynamic-named-ranges-never-manually-updating-your-charts-2/")

fallout.data <- QueueFallout(reportsuite.id, date.from, date.to, metrics, element, checkpoints)

```

### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(fallout.data))
```

```{r}
str(fallout.data)
```


# Example

A typical project flow is:

1. Determine question to be answered via data
2. Scope out the required metrics, props, events etc.
3. Download small sample to test data is in correct format
4. Transform data to shape you need for output
5. Perform statistical modelling etc. 
6. Revisit step 2 if necessary
7. Finalise output with sample data
8. Download full data if necessary
9. Automate scripts if necessary

We demonstrate the above with the question "Did my TV campaign have a significant effect on my SEO traffic?"

## Pose the right question

We want to measure if a statistical significant effect occured to our Natural Search traffic segment when a TV campaign ran from X to Y

## Scope out requirements

We require sessions in the Natural Search traffic segment trended over time.

## Download sample

```{r, message=FALSE, warning=FALSE}
date.from <- "2016-01-01"
date.to <- "2016-09-30"
reportsuite.id <- "grecodemystified"
metrics <- "visits"
segment.id <- "Visit_Natural_Search"
date.granularity <- "day"

overtime.data <- QueueOvertime(reportsuite.id, 
                               date.from, date.to, 
                               metrics,
                               date.granularity = date.granularity,
                               segment.id = segment.id)

```

The data is in this format:

```{R}
str(overtime.data)
```

## Transform data

This is usually the most involved step, but for this simple example we just need the datetime and visit columns

```{r}
## subset to columns we need
model_data <- overtime.data[,c('datetime', 'visits')]

## make datetime a Date object
model_data$datetime <- as.Date(model_data$datetime)
```

A quick exploratory plot:

```{r}
plot(model_data, type = "l")
```

## Statistical modelling

We will now use the [CausalImpact](https://google.github.io/CausalImpact/) package to test the effect of a pretend TV campaign starting around March 10th

```{R, message=FALSE, warning=FALSE}
## install CausalImpact if you don't already have it
## https://google.github.io/CausalImpact/

## load CausalImpact
library(CausalImpact)
## load zoo for working with timeseries
library(zoo)

## turn model_data into a zoo object to work with CausalImpact
model_zoo <- zoo(model_data$visits, order.by = model_data$datetime)

## specify the before and after event segments
## assuming the TV campaign started on 3rd March, 2016
pre.period <- as.Date(c("2016-02-25", "2016-07-10"))
post.period <- as.Date(c("2016-07-11","2016-09-25"))

impact <- CausalImpact(model_zoo, pre.period, post.period) 
```

CausalImpact includes a verbal report and plot to show its findings:

```{r, warning=FALSE}
impact$report

plot(impact)
```

## Review output

Many refinements to the model could be made, for instance:

* Also download control metrics such as direct visits, paid search that may be confounders
* Combine the model data with TV spend data

...but for now we assume the output is valuable and we want to publish.

## Finalise Output

For final display we use a D3 javascript library, [Dygraphs](https://rstudio.github.io/dygraphs/), to display the results:

```{R, message=FALSE, warning=FALSE}
## install dygraphs if necessary
library(dygraphs)

## plot data kept in impact$series
plot_data <- impact$series

dygraph(data=plot_data[,c('cum.effect', 'cum.effect.lower', 'cum.effect.upper')],
        main="Cumulative Impact of Event (95% confidence level)", group="ci") %>%
  dyRangeSelector() %>%
  dyEvent(x = as.Date("2016-03-10"), "TV Campaign") %>%
  dySeries(c('cum.effect.lower', 'cum.effect', 'cum.effect.upper'), label='Cumulative Effect')
```

## Automate?

For automation, the script could run every day to update with the most recent web data, and pull TV campaign dates from another maintained file.

The results could also be made available via a JSON API using a service such as [OpenCPU], for display in other departments dashboards or similar. 

A final step could be to alter marketing spend based on the results of the function - for instance increasing brand PPC spend for TV campaigns that have a poor effect.

# Data strategy for larger data

It may be that the amount of data you require is more than can reasonably be gathered through the API.

In that instance, other strategies used are, in rough order of performance:

1. Export the raw data dump from Adobe Analytics into a database, then use R connectors for that.
    + R is also integrated into Spark, so Hadoop/SparkR integrations are used.
2. Export Data Warehouse exports via secure FTP to a folder, then read the csv files into R.
3. Build robust scripts that retry timeouts from the API and batch and recombine data over a long period.