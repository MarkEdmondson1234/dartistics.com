---
title: "The Adobe Analytics API"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r analytics-apis setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE, cache = FALSE)
library(knitr)
```

Adobe Analytics access is supplied via Randy Zwitch's [RSiteCatalyst](http://randyzwitch.com/RSiteCatalyst/) library. There are _tons_ of explanations and examples in the [content that Randy created for the library](http://randyzwitch.com/RSiteCatalyst/).

Authentication is a little harder to configure for Adobe so a live example is not shown here, but steps to download data are shown below.

# Installation

```r
## This installs a package for downloading Adobe Analytics data
if(!require(RSiteCatalyst)) install.packages("RSiteCatalyst")
```

# Authentication

There are two options for authenticating with Adobe Analytics. The newer way uses OAuth2, which is objectively "better" from a security and general elegance perspective, but it's a bit more involved to set up. The older way simply relies on a couple of keys (a user name and a "secret") and, at some point, will likely be deprecated.

## Authentication - New

This is the new OAuth2 method of authentication.  Use this one if you can.

First, you will need to link your Adobe Analytics login to the Adobe ID account to use this method of signin. 

### Create Application

If you have no application for your Adobe Analytics account, here is how to make one:

1. Create an app by visiting the Adobe developer console and navigating to [Developer > Applications](https://marketing.adobe.com/developer/applications)
2. Select "Web Server (Authorization Code)"
3. Name the application
4. Leave the Redirect Uri field blank.

![](images/setting-up-adobe-application.png)

You should then see this screen to grant permission after running `SCAuth`

![](images/adobe-app-oauth-permission.png)


## Authentication - Old

This is the deprecated method and will dissappear "soon."

Get the Web Service key under the Adobe Analytics Account Information page.

* User Name is used as the key
* Sharted Secret is used as the secret

![](images/legacy-authentication-adobe.png)

If you use this method, then you authenticate via:

```{r message=FALSE, results='hide', error=TRUE}
library(RSiteCatalyst)

key <- Sys.getenv("ADOBE_KEY_OLD")
secret <- Sys.getenv("ADOBE_SECRET_OLD")

SCAuth(key, secret)
```

### Recording secret keys

I save these values to a `.Renviron` file in my home directory to keep them safe and call then via the commands below (this stops them being accidently put on github for example)

As an example, this is the `.Renviron` file in my root directory: `~/.Renviron`

```
ADOBE_KEY="a7xxxxx639-iih-nordic-r"
ADOBE_SECRET="2eadxxxxx1495ea49"
ADOBE_KEY_OLD="mark:XXXXXX"
ADOBE_SECRET_OLD="74b46625xxxxx7e89dbe6e0"
```

# Calling Adobe Analytics

Once you have setup your authentication method, you can call data using the below.

The `SCAuth()` function will open your browser or read the existing `auth-adobe` file if using the newer OAuth2 method, or return `[1] "Credentials Saved in RSiteCatalyst Namespace."` if using legacy.


```r
library(RSiteCatalyst)

## New OAuth method
key    <- Sys.getenv("ADOBE_KEY")
secret <- Sys.getenv("ADOBE_SECRET")

SCAuth(key, secret, 
       company = "XXXXX", 
       token.file = "auth-adobe", 
       auth.method = "OAUTH2")

## Old legacy method
key <- Sys.getenv("ADOBE_KEY_OLD")
secret <- Sys.getenv("ADOBE_SECRET_OLD")

SCAuth(key, secret)

```

You should be then able to see the report suites via:

```r
suites <- GetReportSuites()
head(suites)
```

## Pulling Data Reports


There are 5 different types of Adobe Analytics reports that can be pulled out via the API:

* Overtime - `QueueOvertime()`
* Ranked - `QueueRanked()`
* Trended - `QueueTrended()`
* Pathing - `QueuePathing()`
* Fallout - `QueueFallout()`

See the characteristics of the [reports here](https://marketing.adobe.com/developer/documentation/sitecatalyst-reporting/r-queueovertime)

## Advanced API fetching

If you are not covered by the existing template calls, other API calls can be made via `SubmitJsonQueueReport` if you build the JSON yourself.  It is suggested to use R lists and the library `jsonlite` to create these.

## API response time

* Response times for the API is variable
* It can take anywhere between 10 seconds and 5 minutes or even timeout.
* Size of the requested report doesn't make much impact (within reason)
* Recommend a function to wrap around API calls to retry if an error detected if used in automatic scripts.
* The first call is most expensive - subsequent calls are cached.

# Demo API calls

We set some common parameters here:

```{r}
date.from <- "2016-03-01"
date.to <- "2016-03-8"
reportsuite.id <- "grecodemystified"
```

## Overtime

Returns an overtime report. Or, rather, maybe this is "over time," since it's trending data across a time period, and we're not paying time-and-a-half to the API to get the data.

This is similar to the key metrics report in that the only granularity allowed is time.

QueueOvertime requires a start and end date, a reportsuite ID, and a character vector of metrics.

```{r, warning=FALSE, message=FALSE, results='hide'}
metrics <- c("visits","uniquevisitors","pageviews")

## wrap in system.time to report how long it takes
overtime.data <- QueueOvertime(reportsuite.id, date.from, date.to, metrics)
head(overtime.data)
```

You may also wish to set any of the 5 optional named parameters.

```{r, warning=FALSE, message=FALSE, results='hide'}
metrics <- c("visits","uniquevisitors","pageviews")
date.granularity <- "hour"
segment.id <- "Visit_Natural_Search"
anomaly.detection <- FALSE
data.current <- TRUE
expedite <- FALSE ## only can set to TRUE if you have permission

overtime.data <- QueueOvertime(reportsuite.id, 
                                           date.from, date.to, 
                                           metrics,
                                           date.granularity = date.granularity,
                                           segment.id = segment.id,
                                           anomaly.detection = anomaly.detection,
                                           data.current = data.current,
                                           expedite = expedite)

```


### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(overtime.data))
```

```{r}
str(overtime.data)
```

## Ranked

Returns a ranked report. This is an ordered list of elements and associated metrics with no time granularity.

QueueRanked requires a start and end date, a reportsuite ID, a character vector of elements and a character vector of metrics.

```{r queueranked, message=FALSE, results='hide'}
metrics <- c("visits","uniquevisitors","pageviews","event1")
elements <- c("page","geoCountry","geoCity")

ranked.data <- QueueRanked(reportsuite.id, date.from, date.to, metrics, elements)
```

You may also wish to set any of the 6 optional named parameters. The 1.4 API only supports this for the first element specified. In the example below, selected applies to the first element, page.

```{r queueranked optional, warning=FALSE, message=FALSE, results='hide'}
metrics <- c("visits","uniquevisitors","pageviews","event1","event2")
elements <- c("page","geoCountry","geoCity")
top <- 10
start <- 10
selected <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
              "excel-tips/excel-dynamic-named-ranges-never-manually-updating-your-charts-2/")
segment.id <- ""
data.current <- TRUE
expedite <- FALSE ## only can set to TRUE if you have permission

ranked.data <- QueueRanked(reportsuite.id, 
                           date.from, date.to,
                           metrics,
                           elements,
                           top = top,
                           start = start,
                           selected = selected,
                           segment.id = segment.id,
                           data.current = data.current,
                           expedite = expedite)

unique(ranked.data$page)

```

### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(ranked.data))
```

```{r}
str(ranked.data)
```

## Trended

Returns a trended report. This is an ordered list of elements and associated metrics with time granularity.

QueueTrended requires a start and end date, a reportsuite ID, a character vector of elements and a character vector of metrics.

```{r queuetrended, message=FALSE, results='hide'}
metrics <- c("visits","uniquevisitors","pageviews","event1")
elements <- c("page","geoCountry","geoCity")

trended.data <- QueueTrended(reportsuite.id, date.from, date.to, metrics, elements)

```

You may also wish to set any of the 7 optional named parameters. As with `QueueRanked()` the 1.4 API only supports selected for the first element specified.

```{r queuetrended optional, message=FALSE, results='hide'}
metrics <- c("visits","uniquevisitors","pageviews","event1")
elements <- c("page","geoCountry","geoCity")
top <- 100
start <- 100
selected <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
              "excel-tips/excel-dynamic-named-ranges-never-manually-updating-your-charts-2/")
date.granularity <- "hour"
segment.id <- ""
data.current <- TRUE
expedite <- FALSE

trended.data <- QueueTrended(reportsuite.id, 
                            date.from, date.to,
                            metrics,
                            elements,
                            top = top,
                            start = start,
                            selected = selected,
                            segment.id = segment.id,
                            data.current = data.current,
                            expedite = expedite)
```

### Data format

Report data is returned with the following R classes:

```{r}
## to print in table convert POSIXlt to Date 
trended.data$datetime <- as.Date(trended.data$datetime)
knitr::kable(head(trended.data))
```

```{r}
str(trended.data)
```

## Pathing

Returns a pathing report. This is an ordered list of paths matching the specified pattern.

QueuePathing requires a start and end date, a reportsuite ID, a single element, a single metric and a pattern of element values that defined the path.

```{r pathing, message=FALSE, results='hide'}
metric <- "pageviews"
element <- "page"
pattern <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
             "::anything::",
             "::anything::")

pathing.data <- QueuePathing(reportsuite.id, 
                             date.from, date.to, 
                             metric, 
                             element, 
                             pattern)
```

### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(pathing.data))
```

```{r}
str(pathing.data)
```

## Fallout

Returns a fallout report. This is a pathed list of elements, with fallout values for each.

QueuePathing requires a start and end date, a reportsuite ID, a single element, a character vector of metrics and a character vector of element values that defined the checkpoints.

```{r fallout, message=FALSE, results='hide'}
metrics <- "pageviews"
element <- "page"
checkpoints <- c("excel-tips/excel-dropdowns-done-right-data-validation-and-named-ranges/",
                 "excel-tips/excel-dropdowns-done-right/",
                 "excel-tips/excel-dynamic-named-ranges-never-manually-updating-your-charts-2/")

fallout.data <- QueueFallout(reportsuite.id, date.from, date.to, metrics, element, checkpoints)

```

### Data format

Report data is returned with the following R classes:

```{r}
knitr::kable(head(fallout.data))
```

```{r}
str(fallout.data)
```


# Example

A typical project flow is:

1. Determine question to be answered via data
2. Scope out the required metrics, props, events etc.
3. Download small sample to test data is in correct format
4. Transform data to shape you need for output
5. Perform statistical modelling etc. 
6. Revisit step 2 if necessary
7. Finalise output with sample data
8. Download full data if necessary
9. Automate scripts if necessary

We demonstrate the above with the question "Did my TV campaign have a significant effect on my SEO traffic?"

## Pose the right question

We want to measure if a statistical significant effect occured to our Natural Search traffic segment when a TV campaign ran from X to Y

## Scope out requirements

We require sessions in the Natural Search traffic segment trended over time.

## Download sample

```{r, message=FALSE, warning=FALSE, results='hide'}
date.from <- "2016-01-01"
date.to <- "2016-09-30"
reportsuite.id <- "grecodemystified"
metrics <- "visits"
segment.id <- "Visit_Natural_Search"
date.granularity <- "day"

overtime.data <- QueueOvertime(reportsuite.id, 
                               date.from, date.to, 
                               metrics,
                               date.granularity = date.granularity,
                               segment.id = segment.id)

```

The data is in this format:

```{R}
str(overtime.data)
```

## Transform data

This is usually the most involved step, but for this simple example we just need the datetime and visit columns

```{r}
## subset to columns we need
model_data <- overtime.data[,c('datetime', 'visits')]

## make datetime a Date object
model_data$datetime <- as.Date(model_data$datetime)
```

A quick exploratory plot:

```{r}
plot(model_data, type = "l")
```

## Statistical modelling

We will now use the [CausalImpact](https://google.github.io/CausalImpact/) package to test the effect of a pretend TV campaign starting around March 10th

```{r, message=FALSE, warning=FALSE}
## install CausalImpact if you don't already have it
## https://google.github.io/CausalImpact/

## load CausalImpact
library(CausalImpact)
## load zoo for working with timeseries
library(zoo)

## turn model_data into a zoo object to work with CausalImpact
model_zoo <- zoo(model_data$visits, order.by = model_data$datetime)

## specify the before and after event segments
## assuming the TV campaign started on 3rd March, 2016
pre.period <- as.Date(c("2016-02-25", "2016-07-10"))
post.period <- as.Date(c("2016-07-11","2016-09-25"))

impact <- CausalImpact(model_zoo, pre.period, post.period) 
```

CausalImpact includes a verbal report and plot to show its findings:

```{r, warning=FALSE}
impact$report

plot(impact)
```

## Review output

Many refinements to the model could be made, for instance:

* Also download control metrics such as direct visits, paid search that may be confounders
* Combine the model data with TV spend data

...but for now we assume the output is valuable and we want to publish.

## Finalise Output

For final display we use a D3 javascript library, [Dygraphs](https://rstudio.github.io/dygraphs/), to display the results:

```{R, message=FALSE, warning=FALSE}
## install dygraphs if necessary
library(dygraphs)

## plot data kept in impact$series
plot_data <- impact$series

dygraph(data=plot_data[,c('cum.effect', 'cum.effect.lower', 'cum.effect.upper')],
        main="Cumulative Impact of Event (95% confidence level)", group="ci") %>%
  dyRangeSelector() %>%
  dyEvent(x = as.Date("2016-03-10"), "TV Campaign") %>%
  dySeries(c('cum.effect.lower', 'cum.effect', 'cum.effect.upper'), label='Cumulative Effect')
```

## Automate?

For automation, the script could run every day to update with the most recent web data, and pull TV campaign dates from another maintained file.

The results could also be made available via a JSON API using a service such as [OpenCPU], for display in other departments dashboards or similar. 

A final step could be to alter marketing spend based on the results of the function - for instance increasing brand PPC spend for TV campaigns that have a poor effect.

# Data strategy for larger data

It may be that the amount of data you require is more than can reasonably be gathered through the API.

In that instance, other strategies used are, in rough order of performance:

1. Export the raw data dump from Adobe Analytics into a database, then use R connectors for that.
    + R is also integrated into Spark, so Hadoop/SparkR integrations are used.
2. Export Data Warehouse exports via secure FTP to a folder, then read the csv files into R.
3. Build robust scripts that retry timeouts from the API and batch and recombine data over a long period.

# Generating web_data

Throughout this site, examples and exercise use a data frame called `web_data`. That example data was initially built using Google Analytics data, and instructions for generating that data with your own Google Analytic data are on the [Google Analytics API](http://www.dartistics.com/api-google-analytics.html) page.

But, you're reading about the Adobe Analytics API here, so we can assume you might want to do these same exercises with Adobe Analytics data. To do that, we can use **Last Touch Channel** (Adobe) in lieu of **Default Channel Grouping** (Google Analytics), as well as swap out some other values for  comparable dimensions and metrics. If you are not using **Marketing Channels** in Adobe Analytics, then you will need to adjust the code below by replacing the `lasttouchchannel` dimension with the appropriate dimension.

To do that, first follow the steps for one of the authentication options described earlier on this page. Once that is done, you can use the following overtime command to pull the base data.

```{r base_settings, echo=FALSE, eval=TRUE, message=FALSE, results='hide'}
# Unfortunately, Greco, Inc. doesn't use Marketing Channels, so this cuts
# over to a different account. It'll be a bit of a pain to rebuild this file,
# I realize. Hopefully, we won't need to do that very often!

reportsuite.id <- "mscprod"
key <- Sys.getenv("ADOBE_KEY_OLD_ALT")
secret <- Sys.getenv("ADOBE_SECRET_OLD_ALT")

SCAuth(key, secret)

```


```{r web_data, message=FALSE, results='hide'}

# reportsuite.id <- [uncomment this line and insert your RSID]
date.from <- "2016-01-01"
date.to <- "2016-08-01"
metrics <- c("visits","pageviews","entries","bounces")
elements <- c("mobiledevicetype","lasttouchchannel")
top <- 10
date.granularity <- "day"

web_data <- QueueTrended(reportsuite.id, 
                         date.from, date.to,
                         metrics,
                         elements,
                         top = top)
```

This should give us a table that looks something like this:

```{r raw web_data, echo=FALSE, message=FALSE}
kable(head(web_data))
```



We need to do a little bit of adjustments to the data to, well, Google-ify it -- make it structurally match what's expected for the other examples on this site.

First, let's drop the `segment.id` and `segment.name` columns and rename the columns:

```{r web_data cleanup 1, message=FALSE, results='hide'}
library(dplyr)  # We'll use the select() function from dplyr

web_data <- select(web_data,-segment.id,-segment.name)
colnames(web_data) <- c("date","deviceCategory","channelGrouping","sessions",
                        "pageviews","entrances","bounces")

```

And, now, we need to do a few additional things _just_ to make the data match the sort of data that comes out of a similar query to Google Analytics, which will make the object more compatible with other examples on this site.

```{r web_data cleanup 2, message=FALSE, results='hide'}

# Change the `date` format to be `Date` rather than `POSIXlt.`
web_data$date <- as.Date(web_data$date)

# Change the `Other` value to be `Desktop` for `deviceCategory.`
web_data[web_data$deviceCategory=="Other","deviceCategory"] <- "Desktop"

#Change `Natural Search` to `Organic Search` for `channelGrouping.`
web_data[web_data$channelGrouping=="Natural Search","channelGrouping"] <- "Organic Search"

```

This should give us a table that looks something like this:

```{r cleaned web_data, echo=FALSE, message=FALSE}
kable(head(web_data))
```

And, this _should_ be a data frame that can now be used in other examples on this site.