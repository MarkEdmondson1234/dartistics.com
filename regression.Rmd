---
title: "Regression"
---

Regression is a way to represent cause and effect between two (or more variables).  It attempts to make a best guess or model on how variables influence each other, and gives you an equation which you can use to predict future values.

If you want to just measure how closely two variables are and in which direction, correlation is more appropriate.  However, if you want to try and make predictions from another metric then regression may be the tool for you.

# Types of regression

There are many types of regression for different data scenarios, but we will cover just a few in this introduction that you may need for your own data.

## Simple linear regression

Simple linear regression is performed between two variables, of the form:

```
y = Ax + e
```

Where `y` is the metrics you want to predict (*dependent variable*), `x` a vector of metrics you want to use to predict `y` (*regressors/predictors*), `A` is the matrix of *regression coefficients* of for the variables in `x` and `e` being the bit not predicted, sometimes called *noise*.

In two dimensions this is the same as

```
y = mx + c + e
```

Which you may recognise as the equation for a straight line on a x-y plot, where m is the slope, c is the intercept, and e the random noise. That is pretty much what linear regression is, where the technique of least-squares is used to fit a line to some points.

> An example applied to digital marketing would be predicting transactions from sessions.  The regression coefficient in this case would represent a derived conversion rate.

### Example with Search Console Impressions

If you already have a conversion rate, there isn't much to gain from creating another via linear regression, but in many cases you don't have that linked up view which is where regression can come into play.  An example below looks for a relationship to a website's SEO impressions in Google to the amount of transactions that day.

For this example we shall use the dataset `search_console_plus_ga.csv`, which is two datasets joined on date - SEO impressions and other metrics from Search Console via the `searchConsoleR` package, and Google Analytics total sessions and transactions from `googleAnalyticsR`

```{r regression}
library(knitr) # nice tables
sc_ga_data <- read.csv("./data/search_console_plus_ga.csv", stringsAsFactors = FALSE)

kable(head(sc_ga_data))
```

In R, you create linear regression via the `lm()` function.  We will do so looking for the relationship between `transactions` and `impressions`.

```{r}

model <- lm(transactions ~ impressions, data = sc_ga_data)
```

The model summary takes a bit of getting used to to interpret, but the interesting bits for us are the `r-squared` value and the coefficents

```{r}
library(broom) ## nice tidy model results

## overall model statistics
glance(model)

## coefficients
tidy(model)
```


The `R-squared` value is low which specifies the amount of variation explained by the model is low - only 5% of transactions can be attributed to impressions alone.  This doesn't necessarily mean the model is invalid, but does mean predictions will be less accurate. 

We can say transactions are related to impressions in a measureable way; that `0.00039` transactions correspond to each impression, or `2564` impressions for each transaction.

We plot the data points and the regression line below:

```{r}
## put the linear regression line (not accouting for errors)
plot(sc_ga_data$impressions, sc_ga_data$transactions)
abline(reg = coef(model))
```

## Multi-linear regression

Multi-linear regression is the same as simple linear regression, but you have more predictors - the below shows three (`x, y, ø`) with corresponding *regression coefficients* (`A, B and C`)

```
y = Ax + Bz + Cø + e
```

Plotting a multi-linear regression is harder for more than 3 variables, as you need a multi-dimensional space, but its exactly the same principle - in this case the *coefficients* `A, B and C` show how much `y` would change if their respective *regressor* changed by 1 (and everything else stayed the same)

In R, you create multi-linear regression via the `lm()` function again, but you just add more variables to the model formula:

```{r}
model2 <- lm(transactions ~ impressions + sessions + position, data = sc_ga_data)
## overall model statistics
glance(model2)

## coefficients
tidy(model2)
```

The `r-squared` value has improved a little so we can explain 16% of transactions from our model.

> How do you think the coefficients of multiple linear regression applied to all Google Analytics sessions compare to the actual conversion rate registered in the E-commerce reports?  What about the multi-touch reports?

## Logistic regression

The above regressions are all for continuous data - metrics that can take any value.  They don't work when the thing you want to predict is more categorical e.g. TRUE or FALSE.  An example of this could be predicting if a customer is going to transact in a web session or not.

In R, you create logistic regression by using the `glm()` function and passing in a family argument.  In this case we choose `binomial` (another name for logistic), but the argument takes many other names too for different types of regression - see `?family` for details.

```r
model <- glm(sale ~ saw.promotion + existing.customer + hour.of.day, 
             data = webdata, 
             family = binomial)

```
